{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67260d57-07fd-4894-9f21-89945783992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, DetectorFactory\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b3d6a-2137-4cc1-bb72-0edfe59715d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb586045-c443-471a-9be3-493583e26c45",
   "metadata": {},
   "source": [
    "# Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fec70e-ca90-4f66-80a8-5750faf56283",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data[\"target\"].value_counts()\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046b20a-1473-4178-829a-f24347686fb8",
   "metadata": {},
   "source": [
    "# Get golden standard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c5ab9-de16-4a1e-a1bd-d241deaa469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Natural_disaster\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    article_text = \"\\n\".join([para.get_text() for para in paragraphs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2d588-698b-4999-8f28-0980c19238ea",
   "metadata": {},
   "source": [
    "# Examine language of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36a903-6993-448c-8782-8a6c113c3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0\n",
    "\n",
    "data[\"language\"] = data[\"text\"].apply(lambda text: detect(text))\n",
    "\n",
    "counts = data[\"language\"].value_counts()\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab218f43-447c-440f-8c46-3bf4de2dec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[~(data[\"language\"] == \"en\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7d535-dcc7-4cb5-a2d7-65d709249851",
   "metadata": {},
   "source": [
    "It looks like the \"non-english-texts\" are just incorrectly identified. Most likely all tweets are in english."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592fc62-e3c3-4424-95a2-5e00d90c90af",
   "metadata": {},
   "source": [
    "# Text quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bb10a-9628-47c1-84e1-40b7118c4f99",
   "metadata": {},
   "source": [
    "## Spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b7e0a-7e92-4af9-bef1-7df2ab857253",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8a634-75e5-49fe-ae01-83367a94e5d2",
   "metadata": {},
   "source": [
    "### Wiki article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cdc301-72a9-44ce-96e2-ab27a9c6e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(article_text.split())\n",
    "num_mispelled = len(spell.unknown(article_text.split()))\n",
    "mispelled_ratio = num_mispelled / num_words\n",
    "print(f\"Mispelled ratio for article: {mispelled_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97274946-2f2e-4b77-9fd9-f21519c48b6d",
   "metadata": {},
   "source": [
    "### Disaster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b713c1b-ca59-4073-a770-fb54a686c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"num_words\"] = data[\"text\"].apply(lambda text: len(text.split()))\n",
    "data[\"num_mispelled\"] = data[\"text\"].apply(lambda text: len(spell.unknown(text.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2b9fe-58b7-44eb-a0e0-81be15d10d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data[\"num_mispelled\"]) / sum(data[\"num_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0dcb2-9401-43f8-8a1c-b2db830815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"mispelled_ratio\"] = data[\"num_mispelled\"] / data[\"num_words\"]\n",
    "\n",
    "counts = data[\"mispelled_ratio\"].value_counts()\n",
    "\n",
    "bin_edges = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "data[\"ratio_bins\"] = pd.cut(data[\"mispelled_ratio\"], bins=bin_edges)\n",
    "\n",
    "counts = data[\"ratio_bins\"].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565e4c3-e0df-4d70-8f1f-21a3c9d80b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"mispelled_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dadd2-5cc2-4579-9c46-debe194a02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"mispelled_ratio\", ascending=False)[[\"mispelled_ratio\", \"text\"]].iloc[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f243a1-3d8c-4727-86b4-e8ed513f87fb",
   "metadata": {},
   "source": [
    "## Grammar mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd2b64-2ab6-4210-8b18-ecde27f5220e",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84bdcf-14ba-444c-bdb0-8207e18477c2",
   "metadata": {},
   "source": [
    "# Information content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742087f9-d2dc-4a04-960c-9f1fd2df90b3",
   "metadata": {},
   "source": [
    "## Ratio of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941609d-0523-49b6-b338-2fd92a8c75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stopwords_ratio(text):\n",
    "    words = text.split()\n",
    "    stopwords_count = sum(1 for word in words if word in stop_words)\n",
    "    return stopwords_count / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd904793-8adb-4539-a955-c33ba5c3f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wiki stopwords ratio: {compute_stopwords_ratio(article_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192162af-1af1-4e9d-b6bb-060c0892481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"stopwords_ratio\"] = data[\"text\"].apply(compute_stopwords_ratio)\n",
    "print(f\"Average stopword ratio: {np.mean(data['stopwords_ratio'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9126617-fe46-4051-b92f-edd5644d288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data[\"stopwords_ratio\"].value_counts()\n",
    "\n",
    "bin_edges = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "data[\"ratio_bins\"] = pd.cut(data[\"stopwords_ratio\"], bins=bin_edges)\n",
    "\n",
    "counts = data[\"ratio_bins\"].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acd208-4a18-468c-a465-ba52cb29c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"stopwords_ratio\", ascending=True)[[\"stopwords_ratio\", \"text\"]].iloc[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418473dc-a831-4919-8001-cbfd5339896f",
   "metadata": {},
   "source": [
    "# Average tf-idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f95b58-7acb-46d6-b252-91ec628e39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'text' is the column in your DataFrame that contains the text\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(list(data[\"text\"]) + list(article_text))\n",
    "\n",
    "tfidf_matrix = vectorizer.transform(data[\"text\"])\n",
    "\n",
    "\n",
    "# Compute the average tf-idf score for each text\n",
    "avg_tfidf_scores = np.squeeze(np.asarray(tfidf_matrix.mean(axis=1)))\n",
    "\n",
    "# Append it to the dataframe\n",
    "data[\"avg_tfidf\"] = avg_tfidf_scores\n",
    "\n",
    "# Calculate and print the average tf-idf for the entire dataset\n",
    "dataset_avg_tfidf = np.mean(avg_tfidf_scores)\n",
    "print(f\"Average tf-idf score for the dataset: {dataset_avg_tfidf}\")\n",
    "\n",
    "# 'article_text' is your variable that contains some text in string format\n",
    "# To transform it into a tf-idf vector, we first need to put it in a list\n",
    "article_vect = vectorizer.transform([article_text])\n",
    "\n",
    "# Compute the average tf-idf score for the article\n",
    "article_avg_tfidf = np.mean(article_vect.toarray())\n",
    "print(f\"Average tf-idf score for the article: {article_avg_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca899ea-9840-4d83-afd5-ea2e1f8d24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data[\"avg_tfidf\"].value_counts()\n",
    "\n",
    "bin_edges = np.arange(0, 0.0001, 0.00001)\n",
    "\n",
    "data[\"ratio_bins\"] = pd.cut(data[\"avg_tfidf\"], bins=bin_edges)\n",
    "\n",
    "counts = data[\"ratio_bins\"].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b145bd-41de-4494-909f-ac2ea3d032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"avg_tfidf\", ascending=True)[[\"avg_tfidf\", \"text\"]].iloc[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bc65e-2267-4d81-94b0-1f2956bf4e96",
   "metadata": {},
   "source": [
    "## Unknown BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17213f-8c6c-4fc8-b1f6-64131149a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def count_unknowns(text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    unknown_tokens = [token for token in tokenized_text if token == \"[UNK]\"]\n",
    "    return len(unknown_tokens)\n",
    "\n",
    "\n",
    "data[\"unknowns_count\"] = data[\"text\"].progress_apply(count_unknowns)\n",
    "\n",
    "\n",
    "average_unknowns = data[\"unknowns_count\"].mean()\n",
    "print(f\"Average number of unknown tokens in texts: {average_unknowns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a39db-a6a0-4366-9780-7e18e5e08a03",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd515e-57f1-4f85-8790-a79a1f14a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(data['text'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "\n",
    "embeddings_np = embeddings.numpy()\n",
    "\n",
    "clusters = kmeans.fit_predict(embeddings_np)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use t-SNE to reduce the dimensionality of the embeddings to 2D\n",
    "embeddings_2d = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "# `embeddings_2d` is a 2D tensor where each row is a 2D representation of a text in your dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame for seaborn\n",
    "plot_data = pd.DataFrame(embeddings_2d, columns=[\"Dim1\", \"Dim2\"])\n",
    "plot_data['Target'] = data['target'].values  # This assumes your \"target\" column is accessible here\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=plot_data, x=\"Dim1\", y=\"Dim2\", hue=\"Target\", palette=\"deep\")\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19643f7-a9f8-48d2-9fe2-171009031fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035a22f-5a16-4254-b21c-c7a3e2ee6d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
